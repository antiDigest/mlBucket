1. For MLE (or EM M-step), since the parameter w of singleton potential is shared across all the random variables (edges in the original graph), each feature function is the sum of color indicator function over all the random variables . Specifically, there are K such features, f_k(X) = \sum_i 1(x_i==k), k=1,...,K, where X is the joint assignment of all random variables x_i, and 1(x_i==k) is 1 if x_i takes on the kth color and 0 otherwise. 

Also remember there's no need to deal with the non-singleton cliques, as their weights are already implicitly set by the coloring constraints (see https://piazza.com/class/jbnxvxz1y8d3ps?cid=74 https://piazza.com/class/jbnxvxz1y8d3ps?cid=82) and has nothing to do with w.

2. For EM E-step, you need the marginal probabilities p(x_h=k | X_o^(m)), for each hidden variable x_h, conditioned on the mth incomplete sample X_o^(m). This requires running inference (e.g. using BP) over the original Markov network while clamping each of the variables in X_o^(m) to their observed values (which induces a reduced Markov network over X_h only). This can be easily achieved by multiplying the singleton potential of each variable x_oi \in X_o^(m) with an indicator function 1(x_oi==x_oi^(m)), so that we redefine phi(x_oi) by phi(x_oi):=phi(x_oi)*1(x_oi==x_oi^(m)), then running BP as usual (see my cond_lbp2.m for an implementation). The relevant concepts are explained in Koller text section 4.2.3; also a thorough discussion here: http://people.eecs.berkeley.edu/~jordan/prelims/chapter3.pdf.

The test case is a simple 3-node chain; for EM, only one node is hidden. The ground truth weight is [-1 0 1]; since the parameterization is redundant, there's no unique solution, and your answer may be shifted (this depends on your initialization) by a constant like [2.9982 4.0220 4.9798] which is totally fine.
